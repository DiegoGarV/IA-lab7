{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 7\n",
    "\n",
    "## Task 1\n",
    "\n",
    "1. **¿Qué es el temporal difference learning y en qué se diferencia de los métodos tradicionales de aprendizaje supervisado? Explique el concepto de \"error de diferencia temporal\" y su papel en los algoritmos de aprendizaje por refuerzo**<br>\n",
    "El TD Learning es un método de aprendizaje por refuerzo donde el agente no necesita un modelo explícito del entorno o esperar el resultado final ed un episodio para aprender los valores del estado. Se diferencia del aprendizaje supervisado tradicional con que el TD interactuá de forma directa con el entorno y aprende de su propia experiencia. El \"error de diferencia temporal\" o \"TD Error\" es el que mide la diferencia entre la estimación actual de la recompensa futura y la recompensa real obtenida. Este es importante para el TD learning porque es el que le premite ir aprendiendo de su experiencia, sirviendole como una señal de corrección. \n",
    "\n",
    "2. **En el contexto de los juegos simultáneos, ¿cómo toman decisiones los jugadores sin conocer las acciones de sus oponentes? De un ejemplo de un escenario del mundo real que pueda modelarse como un juego simultáneo y discuta las estrategias que los jugadores podrían emplear en tal situación.**<br>\n",
    "Normalmente, para un juego simultaneo, se hacen supocisiones sobre la capacidad de juego del oponente y en base a eso se le asigna una probabilidad a cada decisión. En base a esto ya se llega a un movimiento. Por ejemplo, en el juego de Super Smash Bros (juego de peleas de Nintendo) ambos jugares se enfrentan simultaneamente y no saben cuál será el movimiento de su adversario. Lo más normal en estas situaciones es ver un poco de como juega el oponente y ver que movimientos son los que más usa y a cuales le es más difícil contraatacar. Ya con esto podemos hacer una estrategia para cubrirnos de sus ataques y podes castigarlo de mejor forma, incluso llegando a hacer predicts de sus movimientos. \n",
    "\n",
    "3. **¿Qué distingue los juegos de suma cero de los juegos de NO suma cero y cómo afecta esta diferencia al proceso de toma de decisiones de los jugadores? Proporcione al menos un ejemplo de juegos que entren en la categoría de juegos de no suma cero y discuta las consideraciones estratégicas únicas involucradas**<br>\n",
    "La diferencia es que en los de suma cero, las ganancias de un jugador son las perdidas del otro. Por otro lado, en uno de *no* suma cero, la suma de la recompensa de ambos no da exactamente 0, ya que estos pueden llegar a colaborar. Un ejemplo de esto es el dilema del prisionero. En este, dos sospechosos son interrogados de forma separada y pueden escoger entre colaborar (no delatar) o traicionar (si delatar). Si ambos guardan silencio, ambos reciben una pena menor. Si uno delata y el otro no el que delató queda libre y el otro recibe la peor pena. Si ambos delatan ambos reciben una pena media. La estrategia para este tipo de juegos va de buscar comunicarse con el oponente para llegar al mejor resultado para ambos (como en los programas de TV donde se reparten dinero). Pero esto no siempre es una opción, por lo que también es posible ir cambiando entre ser cooperativo o no según el comportamiento del oponente.\n",
    "\n",
    "4. **¿Cómo se aplica el concepto de equilibrio de Nash a los juegos simultáneos? Explicar cómo el equilibrio de Nash representa una solución estable en la que ningún jugador tiene un incentivo para desviarse unilateralmente de la estrategia elegida**<br>\n",
    "El equilibrio de Nash es un estado donde los jugadores no tienen una razón para cambiar su estrategia. Esto pasa cuando cada jugador escogen la estrategia que creen óptima con base a lo que creen que van a hacer los otros, por lo tanto ninguno se beneficia de cambiarla. Utilizando el ejemplo del prisionero, si un jugador cree que el otro no va a confezar entonces este confieza para quedar libre. Sí, en cambio, cree que el otro va a confezar igual confienza, ya que le combiene más quedar con una pena media que con una alta. Entonces, el equilibrio de Nash se ve cuando ninguno de los jugadores puede obtener una mayor recompensa solamente cambiando su estrategia. Este no es necesariamente la mejor opción para todos pero sí libra de riesgos mayores a cada jugador.\n",
    "\n",
    "5. **Discuta la aplicación del temporal difference learning en el modelado y optimización de procesos de toma de decisiones en entornos dinámicos. ¿Cómo maneja el temporal difference learning el equilibrio entre exploración y explotación y cuáles son algunos de los desafíos asociados con su implementación en la práctica?**<br>\n",
    "El TD Learning en entornos dinámicos permite actualiza los valores de estado sin la necesidad de que termine el estado. También se adapta a los cambios dinámicos del entorno actualizando cada vez más los valores de acción. Además, es capaz de funcionar en problemas donde el entorno no tenga un modelo definido. Cuando se trata de exploración y explotación, el TD Learning utiliza un enfoque greedy, escogiendo acciones aleatorias para explorar y luego basado en eso crea una estrategia óptima (explotación) si por alguna razón su estrategía deja de ser óptima este vuelve a explorar diferentes estrategias hasta llegar al mejor resultado. Así, va actualizando solo sus estrategias sin necesidad de tener conocimiento de todo lo que ocurrer desde el inicio. Aún así, en la práctica se pueden dar varios desafíos. Uno de ellos es que cada si los estados son muy grandes, la exploración puede llegar a ser muy lenta. Otro problema puede darse si existe alguna falla en su exploración, ya que este va utilizando sus estimaciones previas y si en alguna de estas hay un error es probable que este se arrastre. También pueden darse errores si las recompensas o estados cambian con el tiempo, ya que esto hará que el modelo necesite reajustes manuales.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
